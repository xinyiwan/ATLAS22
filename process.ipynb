{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "# TODO Transfer learning & Main architecture\n",
    "#     Lab5 - task2 dice 0.95    \n",
    "#     model  saved in ‘/home/k8s-group5’\n",
    "#     Load data function (try Mehdi’s) no shuffling\n",
    "#     UNET 2D\n",
    "\n",
    "# Model for transfer learning input (model, '/home/k8s-group5')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, SpatialDropout2D,Conv2DTranspose,Concatenate\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/home/k8s-group5/assets/'\n",
    "\n",
    "def TL_unet_model(input_shape):\n",
    "\n",
    "    input_shape = input_shape\n",
    "    base = model_dir  #TODO change to the actual model, how do I import as model?\n",
    "\n",
    "    # base = VGG16(include_top = False, \n",
    "    #                weights = \"imagenet\", \n",
    "    #                input_shape = input_shape)\n",
    "\n",
    "\n",
    "    # freezing all layers in VGG16 \n",
    "    for layer in base.layers: \n",
    "        layer.trainable = False\n",
    "\n",
    "    # the bridge (exclude the last maxpooling layer in VGG16) \n",
    "    bridge = base.get_layer(\"block5_conv3\").output #TODO change layer\n",
    "    print(bridge.shape)\n",
    "\n",
    "    # Decoder now\n",
    "    up1 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(bridge)\n",
    "    print(up1.shape)\n",
    "    concat_1 = Concatenate([up1, base.get_layer(\"block4_conv3\").output], axis=3) #TODO change layer\n",
    "    conv6 = Conv2D(512, (3, 3), activation='relu', padding='same')(concat_1)\n",
    "    conv6 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up2 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv6)\n",
    "    print(up2.shape)\n",
    "    concat_2 = Concatenate([up2, base.get_layer(\"block3_conv3\").output], axis=3) #TODO change layer\n",
    "    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(concat_2)\n",
    "    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up3 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv7)\n",
    "    print(up3.shape)\n",
    "    concat_3 = Concatenate([up3, base.get_layer(\"block2_conv2\").output], axis=3) #TODO change layer\n",
    "    conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(concat_3)\n",
    "    conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up4 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv8)\n",
    "    print(up4.shape)\n",
    "    concat_4 = Concatenate([up4, base.get_layer(\"block1_conv2\").output], axis=3) #TODO change layer\n",
    "    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(concat_4)\n",
    "    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "    print(conv10.shape)\n",
    "\n",
    "    model_ = Model(inputs=[base.input], outputs=[conv10])\n",
    "\n",
    "    return model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(x_train, y_train, batch_size):\n",
    "    n_train_sample = len(x_train)\n",
    "    while True:\n",
    "               \n",
    "        for ind in (range(0, n_train_sample, batch_size)):\n",
    "            \n",
    "            batch_img = x_train[ind:ind+batch_size]\n",
    "            batch_label = y_train[ind:ind+batch_size]\n",
    "            \n",
    "            # Sanity check assures batch size always satisfied\n",
    "            # by repeating the last 2-3 images at last batch.\n",
    "            length = len(batch_img)\n",
    "            if length == batch_size:\n",
    "                pass\n",
    "            else:\n",
    "                for tmp in range(batch_size - length):\n",
    "                    batch_img = np.append(batch_img, np.expand_dims(batch_img[-1],axis=0), axis = 0)\n",
    "                    batch_label = np.append(batch_label, np.expand_dims(batch_label[-1], axis=0), axis = 0)\n",
    "        \n",
    "            backgound_value = x_train.min()\n",
    "            data_gen_args = dict(rotation_range=10.,\n",
    "                                     width_shift_range=0.1,\n",
    "                                     height_shift_range=0.1,\n",
    "                                     cval = backgound_value,\n",
    "                                     zoom_range=0.2,\n",
    "                                     horizontal_flip = True)\n",
    "            \n",
    "            image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "            mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "            \n",
    "            image_generator = image_datagen.flow(batch_img, shuffle=False,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 seed=1)\n",
    "            \n",
    "            mask_generator = mask_datagen.flow(batch_label, shuffle=False,\n",
    "                                               batch_size=batch_size,\n",
    "                                               seed=1)\n",
    "            \n",
    "            image = image_generator.next()\n",
    "            label = mask_generator.next()\n",
    "            \n",
    "            yield image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\corne\\OneDrive\\Documents\\KTH\\ATLAS22\\process.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/corne/OneDrive/Documents/KTH/ATLAS22/process.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m input_shape \u001b[39m=\u001b[39m shape\u001b[39m=\u001b[39m(\u001b[39m100\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/corne/OneDrive/Documents/KTH/ATLAS22/process.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/c%3A/Users/corne/OneDrive/Documents/KTH/ATLAS22/process.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m TL_unet_model(input_shape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/corne/OneDrive/Documents/KTH/ATLAS22/process.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/corne/OneDrive/Documents/KTH/ATLAS22/process.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_generator \u001b[39m=\u001b[39m generator(image_train, mask_train, batch_size)\n",
      "\u001b[1;32mc:\\Users\\corne\\OneDrive\\Documents\\KTH\\ATLAS22\\process.ipynb Cell 4\u001b[0m in \u001b[0;36mTL_unet_model\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/corne/OneDrive/Documents/KTH/ATLAS22/process.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m base \u001b[39m=\u001b[39m model_dir \u001b[39m#TODO change to the actual model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/corne/OneDrive/Documents/KTH/ATLAS22/process.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# base = VGG16(include_top = False, \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/corne/OneDrive/Documents/KTH/ATLAS22/process.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#                weights = \"imagenet\", \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/corne/OneDrive/Documents/KTH/ATLAS22/process.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#                input_shape = input_shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/corne/OneDrive/Documents/KTH/ATLAS22/process.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/corne/OneDrive/Documents/KTH/ATLAS22/process.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# freezing all layers in VGG16 \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/corne/OneDrive/Documents/KTH/ATLAS22/process.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m base\u001b[39m.\u001b[39;49mlayers: \n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/corne/OneDrive/Documents/KTH/ATLAS22/process.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     layer\u001b[39m.\u001b[39mtrainable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/corne/OneDrive/Documents/KTH/ATLAS22/process.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# the bridge (exclude the last maxpooling layer in VGG16) \u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'layers'"
     ]
    }
   ],
   "source": [
    "input_shape = shape=(100, 100, 1)\n",
    "batch_size = 8\n",
    "\n",
    "model = TL_unet_model(input_shape)\n",
    "model.summary()\n",
    "train_generator = generator(image_train, mask_train, batch_size)\n",
    "val_generator = generator(image_val, mask_val, batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71fff6981f968a4cb9b01dea9d842cfdb430404d76e93138a7f2901e0447470a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
